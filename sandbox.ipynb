{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CUDA\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {str(DEVICE).upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_DIR = \"/mnt/artemis/library/weights/meta/llama-2/7Bf/hf\"\n",
    "MODEL_DIR = \"/mnt/artemis/library/weights/mistral/OpenHermes-2-Mistral-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.40s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 12948, 13334,   381, 27613,  3339,   477,   272, 16578,   304,\n",
      "          3395,   378,   298,   676, 28723]], device='cuda:0')\n",
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "en1 = \"Prometheus stole fire from the gods and gave it to man.\"\n",
    "# en1 = \"What is 2+2=?\"\n",
    "tok = tokenizer(en1, return_tensors='pt').to(DEVICE).input_ids\n",
    "print(tok)\n",
    "print(tok.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = torch.tensor([1] * len(tok[0])).unsqueeze(0).to(DEVICE)\n",
    "print(attn_mask.shape)\n",
    "\n",
    "tok = model.generate(\n",
    "    input_ids = tok,\n",
    "    attention_mask = attn_mask,\n",
    "    max_new_tokens = 512,\n",
    "    do_sample = True,\n",
    "    top_p = 1.0,\n",
    "    top_k = 10,\n",
    "    temperature = 0.7,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    repetition_penalty = 1.0,\n",
    "    length_penalty = 1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prometheus stole fire from the gods and gave it to man. In return he was punished by being chained to a rock where an eagle would eat his liver each day. His liver would grow back, so the eagle could eat it again the next day. This punishment was repeated for eternity.\n",
      "\n",
      "Prometheus was punished for his defiance of the gods and for stealing fire, a symbol of knowledge and power. This story can be used to illustrate the dangers of defying authority and the consequences of taking risks in the pursuit of knowledge.\n",
      "\n",
      "Prometheus also represents the archetype of the trickster, who uses his wit and cunning to outsmart those in power. This archetype is often associated with creativity and innovation, as the trickster figure is willing to take risks and break rules in order to achieve their goals.\n",
      "\n",
      "In a broader sense, the story of Prometheus can be seen as a cautionary tale about the dangers of playing with fire, both literally and metaphorically. It warns against the dangers of seeking knowledge and power for its own sake, and the consequences of defying authority in the pursuit of those goals."
     ]
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(\n",
    "    sequences = tok,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    padding_side='left')[0]\n",
    "\n",
    "for word in response.strip():\n",
    "    print(word, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
